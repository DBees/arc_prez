---
title: "Advanced Reasoning Corpus"
author: "Matthew Emery"
date: "7/4/2020"
output: revealjs::revealjs_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## TODOs

 - Bibliography
 - Update CSS

## Table of Contents

 - What is intelligence?
 - Introducing the Advanced Reasoning Corpus
 - Kaggle Solutions

## What is the goal of AI Research?

> AI is the science of making machines capable of performing tasks that would require intelligence if done by humans. - Marvin Minsky

<!-- Circular argument -->


## What is the goal of Advanced Reasoning Corpus (ARC)?

> "ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans."

<!-- picture of Francois --> 

## Weaknesses of AI Research

 - Current algorithms are narrow and data hungry
 - Turing Test relies on human judges
 - Lack of agreement biases research to narrow, well defined skills

## Defining Intelligence Currently

From Legg and Hutter:

> Intelligence measures an agentâ€™s ability to *achieve goals* in a *wide range of environments*.

1. Achieve goals 
2. Wide range of environments (Adaptability and Generalization)

Chollet's argument: We've spent too much time on 1.

## What would a good AI adaptability benchmark have?

  - Reproducibility
  - Fairness
  - Scalability
  - Flexibility


## AI Effect
<!-- Should I keep? -->

McCorduck [61] AI Effect: "Every time somebody figured out how to make a computer do something...there was a chorus of critics to say 'that 's not thinking"

## Two ways of thinking about generalization

1. System-centric generalization
  - Most machine learning is here
  - The developer tries to compensate for data not in the dataset
  - Known unknowns

2. Developer-aware generalization
  - Generalizations beyond what the developer expected
  - Unknown unknowns

[](img/impact.gif)


## The generalization spectrum

 1. Absence of generalization: Classical algorithms. No uncertainty.
 2. Local generalization Most machine learning
 3. Broad generalization: Wozniuk's coffee cup challenge
 4. **Extreme generalization:** Human-level intelligence
 5. Universal generalization: Impossible due to the [[No Free Lunch Theorem]]

## Psychometrics Perspective

 - There's a whole field dedicated to measuring intelligence called psychometrics

![](img/psychometrics.png)

## G Factor

- IQ tests are very human centric
- Why not consider octopus camoflauge intelligence?
- Think of G Factor like general athleticism
- There are limits to athletic measurement, we wouldn't measure humans at the bottom of the ocean or Mars
- Humans are incredible efficient at solving 2D and 3D problems but we are terrible at 4D problems

<!-- Why don't we give AI IQ tests? There are too many human centric assumptions -->

## Principles of an AGI Test

- Measure abilities, not skills
- Use a battery of tests, not one
- Set standards on reliability, validity and open standards
- Remove the need for human-level knowledge (bias)
- Focus on skill acquisition efficiency, not skills themselves

## Current trends in AGI evaluation

- Look at data efficiency in RL
- Most RL tests don't measure reobustness
- Can sample arbitrary amounts of data
- OpenAI Five was brittle to exploits

![](img/openai5cheese.png)

## Measuring the right thing

- DeepBlue didn't get us much closer to AGI
- Neither did the video game challenges
- Real life is a chaotic system, we need to optimize adaptability
- Often, the video game challenges are measuring the ML engineer's intelligence, not the system itself
- Note that a k-nearest neighbour algorithm can solve any task, given enough data

## Priors

- Human intelligence is not hard-coded, but we also aren't blank slates
- To compare intelligences we need to control for experience, priors and difficulty of the task
- Some human priors
    - Reflex priors (uninteresting to us for this challenge)
    - Metalearning priors (These are the priors we are trying to reverse engineer)
    - High level knowledge priors (e.g. object permenance)
- Machines lack these priors but have hard-coded knowledge

## Priors

- We should make the machine priors as close to human priors as possible
- The less priors you have, the more difficulty the disadvantaged the machine is
- **All priors should be enumerated**

### ARC Priors
- Objectness and elementary physics
    - Object recognition
    - Cohesion
    - Persistance
    - Contact

- Agentness
    - Some objects act **contigently** and **reciprocally**

- Natural numbers up to 10
    - comparison
    - sorting
    - addition and subtraction

- Geoemtry/Topology
    - Distance
    - Orientation
    - In/Out relantionships

All of these should be hard-coded into an AI as a domain specific language

## Francois Chollet's Definition of Intelligence

> "The intelligence of a system is a measure of it's skill acquisition efficiency over a scope of tasks with respect to priors, experience, and generalization difficulty"


## Position of the Problem
![](/home/deadhand/Documents/matt-zettlr/img/arc1.png)
- {Insert schematic here}
- Task gives a situation to the skill program and receives a response in return. The task then gives feedback to the intelligent system and a score. The task then updates
- Intelligent system (IS) generates a skill program and receives feedback from the task. The intelligent system updates on getting feedback
- Skill program receives a situation from the task and returns a response. Continues until receiving a STOP situation

### The evaluation phase
- The Intelligent System creates a skill program and then can no longer interact with the system
- IS receives a sum of scaled scores
- IS needs a sufficient skill threshold, the minimum skill needed to solve a task
- Cirriculum, the sequence of training examples the skill program interacts with

## Algorithmic Information Theory
- Measures the information content of a math objects
- Algorithmic Complexity $$H(s)$$: shortest Turing machine to get a solution
Example, all even numbers less than 10000000
- Relative Algorithmic Complexity $$H(s_1|s_2)$$: The shortest turing machine instructions if $$s_2$$ given
- Generalization difficulty: 
$$G D_{T, C}^{\theta}=\frac{H\left(\operatorname{Sol}_{T}^{\theta} \mid \operatorname{TrainSol}_{T, C}^{o p t}\right)}{H\left(\operatorname{Sol}_{T}^{\theta}\right)}$$
The short solution to task T given the shortest optimal train time solution given by cirriculum C
If the optimal train time solution solves Sol, the generalization is 0
Example: A nearest neighbor algo vs. single cut off

## Developer Aware Generalization Difficulty
$$G D_{I S, T, C}^{\theta}=\frac{H\left(S o l_{T}^{\theta} \mid \text {TrainSol}_{T, C}^{\text {opt}}, I S_{t=0}\right)}{H\left(\operatorname{Sol}_{T}^{\theta}\right)}$$
This now accounts for the IS's priors

## Experience
$$E_{I S, T, t}^{\theta}=H\left(S o l_{T}^{\theta} \mid I S_{t}\right)-H\left(S o l_{T}^{\theta} \mid I S_{t}, d a t a_{t}\right)$$
Over cirriculym C:
$$E_{I S, T, C}^{\theta}=\frac{1}{H\left(S o l_{T}^{\theta}\right)} \sum_{t} E_{I S, T, t}^{\theta}$$
- If the cirriculum is noisy or repetitive the algorithm is not punished
- Experience is the amount of relevant, novel information in the system

## Putting it all together
$$I_{I S, s c o p e}^{\theta_{T}}=\underset{T \in s c o p e}{A v g}\left[\omega_{T} \cdot \theta_{T} \sum_{C \in C u r_{T}^{\theta_{T}}}\left[P_{C} \cdot \frac{G D_{I S, T, C}^{\theta_{T}}}{P_{I S, T}^{\theta_{T}}+E_{I S, T, C}^{\theta_{T}}}\right]\right]$$
We are trying to maximize this value
Adding priors and experience reduced the intelligence
Generalization difficulty increases it
$\omega_T$ is the subjective measure of skill involved
- intelligence is tried to scope
- A better cirriculum can increase intelligence

### Alternative Efficiencies
- Computational efficiency of skill programs/intelligent systems
- Time efficiency
- Energy efficiency (the brain uses less energy than computers)
- Risk efficiency (could be important if IS interacting with the real world)

## Pracitical Implications
- Creating an intelligent system can be seen as an optimization problem
- Focuses on broad abilities
- Focuses on [[program synethesis]]
- Focus on cirriculum development
- Defines generalization levels
- Can compare humans to AI

### Fair evaluation between Intelligent Systems
- Scope must be well-defined
- Focus on skill-efficiency not  maxmium skill
- Only compare intelligence of systems with similar priors

### Expectations of an ideal intelligence benchmark
- Describe scope of applications and provide validity
- Should be reproduible
- Should have developer-aware generalization
- Should quanitify GD (ARC doesn't)
- Should account for data
- Should exhaustively describe priors
- Should be available to both humans and machines

## The ARC Dataset
- Similar to Raven's Progressive Matrices
- Broad
- Developer-aware
- Experience Controlled
- Priors stated
- 400 training tasks/400 public evaluations/200 private

### ARC description
- Up to 10 unique symbols/colors
- Size from 1x1 to 30x30
- Usually 3 trials per test
- A typical human can solve most ARC problems with no previous training

## ARC Core Knowledge Priors
### Objectness
- Cohesion
- Persistance
- Influence on Contact

### Number and Counting Priors
- Can count, sort and compare
- Addition and subtraction

### Geometry/Topology
- Rectangles
- Symmetries
- Upscaling/Downscaling
- Drawing Lines
- Copying

### Differences with Psychometric Tests
- No crystallized knowledge (like NLp or object recognition)
- Greater task diversity
- ARC challenges are not generated programmatically

### What would a possible intelligent system look like?
- At least 1 and 3 high IQ humans were able to solve each task
- Need more human data for a fair comparison
- Deep learning won't work here
- Program synthesis important
- Make a Domain Specific Language (DSL) to describe all possible situations
- Generate candidate programs
- Select top 3 candidates

### Weaknesses & Future Refinement
- ARC solve could have human-like intelligent, or not!
- Generalization difficult not quantified
- Test validaty not established
- Dataset diversity is limited
- Evalutaion is overly close-ended
- Priors may not be well captured in ARC

### Possible Alternatives
- Depend on other benchmarks
- Open-ended or collaborative approaches

### Taking stock
- Intelligence is efficiency of learning
- Need to account for priors, experience and generalization difficulty

# When do you think an algorithm will 

## Bibliography
